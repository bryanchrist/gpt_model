{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5294d9e-a162-4eb6-ac89-e153b99f7ee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code here is borrowed from the Karpathy github @ https://github.com/karpathy/ng-video-lecture/blob/master/gpt.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ed070-5a24-431d-a1bd-33d79198932d",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a5345ff-fcdb-43bb-a8f7-6ed3a1005c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cb57be-878f-427a-b83c-0f0041dbfe08",
   "metadata": {},
   "source": [
    "# Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "495a6a1f-455b-49c3-9cc2-7ec6be861452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdef884a4b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64 # how many independent sequences will we process in parallel?\n",
    "block_size = 280 # amended to be max length of a tweet\n",
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb9b7cd2-b6b9-4dd7-8c18-c4190f492a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt_model.ipynb',\n",
       " 'LICENSE',\n",
       " 'IRAhandle_tweets_1.csv',\n",
       " 'README.md',\n",
       " 'input.txt',\n",
       " '.gitignore',\n",
       " '.ipynb_checkpoints',\n",
       " '.git']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003ec968-3007-4e80-be62-49e1a4c40e26",
   "metadata": {},
   "source": [
    "# Load Data and Set Up Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cd4a216-d2c8-4bf7-ae66-b89b250ff425",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IRAhandle_tweets_1.csv')\n",
    "df = df[df['language']==\"English\"]\n",
    "tweets = []\n",
    "for i in range(0, len(df)):\n",
    "    tweet = df.iloc[i]['content']\n",
    "    tweet = tweet.split('http')[0]\n",
    "    tweets.append(tweet)\n",
    "text = \"/n\".join(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55578436-81b9-40d9-a0d2-4f1a39e73fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>external_author_id</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>region</th>\n",
       "      <th>language</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>harvested_date</th>\n",
       "      <th>following</th>\n",
       "      <th>followers</th>\n",
       "      <th>updates</th>\n",
       "      <th>...</th>\n",
       "      <th>account_type</th>\n",
       "      <th>retweet</th>\n",
       "      <th>account_category</th>\n",
       "      <th>new_june_2018</th>\n",
       "      <th>alt_external_id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>article_url</th>\n",
       "      <th>tco1_step1</th>\n",
       "      <th>tco2_step1</th>\n",
       "      <th>tco3_step1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>906000000000000000</td>\n",
       "      <td>10_GOP</td>\n",
       "      <td>\"We have a sitting Democrat US Senator on tria...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>English</td>\n",
       "      <td>10/1/2017 19:58</td>\n",
       "      <td>10/1/2017 19:59</td>\n",
       "      <td>1052</td>\n",
       "      <td>9636</td>\n",
       "      <td>253</td>\n",
       "      <td>...</td>\n",
       "      <td>Right</td>\n",
       "      <td>0</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>0</td>\n",
       "      <td>905874659358453760</td>\n",
       "      <td>914580356430536707</td>\n",
       "      <td>http://twitter.com/905874659358453760/statuses...</td>\n",
       "      <td>https://twitter.com/10_gop/status/914580356430...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>906000000000000000</td>\n",
       "      <td>10_GOP</td>\n",
       "      <td>Marshawn Lynch arrives to game in anti-Trump s...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>English</td>\n",
       "      <td>10/1/2017 22:43</td>\n",
       "      <td>10/1/2017 22:43</td>\n",
       "      <td>1054</td>\n",
       "      <td>9637</td>\n",
       "      <td>254</td>\n",
       "      <td>...</td>\n",
       "      <td>Right</td>\n",
       "      <td>0</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>0</td>\n",
       "      <td>905874659358453760</td>\n",
       "      <td>914621840496189440</td>\n",
       "      <td>http://twitter.com/905874659358453760/statuses...</td>\n",
       "      <td>https://twitter.com/damienwoody/status/9145685...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>906000000000000000</td>\n",
       "      <td>10_GOP</td>\n",
       "      <td>Daughter of fallen Navy Sailor delivers powerf...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>English</td>\n",
       "      <td>10/1/2017 22:50</td>\n",
       "      <td>10/1/2017 22:51</td>\n",
       "      <td>1054</td>\n",
       "      <td>9637</td>\n",
       "      <td>255</td>\n",
       "      <td>...</td>\n",
       "      <td>Right</td>\n",
       "      <td>1</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>0</td>\n",
       "      <td>905874659358453760</td>\n",
       "      <td>914623490375979008</td>\n",
       "      <td>http://twitter.com/905874659358453760/statuses...</td>\n",
       "      <td>https://twitter.com/10_gop/status/913231923715...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>906000000000000000</td>\n",
       "      <td>10_GOP</td>\n",
       "      <td>JUST IN: President Trump dedicates Presidents ...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>English</td>\n",
       "      <td>10/1/2017 23:52</td>\n",
       "      <td>10/1/2017 23:52</td>\n",
       "      <td>1062</td>\n",
       "      <td>9642</td>\n",
       "      <td>256</td>\n",
       "      <td>...</td>\n",
       "      <td>Right</td>\n",
       "      <td>0</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>0</td>\n",
       "      <td>905874659358453760</td>\n",
       "      <td>914639143690555392</td>\n",
       "      <td>http://twitter.com/905874659358453760/statuses...</td>\n",
       "      <td>https://twitter.com/10_gop/status/914639143690...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>906000000000000000</td>\n",
       "      <td>10_GOP</td>\n",
       "      <td>19,000 RESPECTING our National Anthem! #StandF...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>English</td>\n",
       "      <td>10/1/2017 2:13</td>\n",
       "      <td>10/1/2017 2:13</td>\n",
       "      <td>1050</td>\n",
       "      <td>9645</td>\n",
       "      <td>246</td>\n",
       "      <td>...</td>\n",
       "      <td>Right</td>\n",
       "      <td>1</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>0</td>\n",
       "      <td>905874659358453760</td>\n",
       "      <td>914312219952861184</td>\n",
       "      <td>http://twitter.com/905874659358453760/statuses...</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/914...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243886</th>\n",
       "      <td>2497991305</td>\n",
       "      <td>AUSTINLOVESBEER</td>\n",
       "      <td>BREAKING: Killer avalanche sweeps three skiers...</td>\n",
       "      <td>United States</td>\n",
       "      <td>English</td>\n",
       "      <td>3/8/2017 8:59</td>\n",
       "      <td>3/8/2017 8:59</td>\n",
       "      <td>41</td>\n",
       "      <td>34</td>\n",
       "      <td>173</td>\n",
       "      <td>...</td>\n",
       "      <td>Right</td>\n",
       "      <td>1</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>0</td>\n",
       "      <td>2497991305</td>\n",
       "      <td>839400198002503680</td>\n",
       "      <td>http://twitter.com/2497991305/statuses/8394001...</td>\n",
       "      <td>https://twitter.com/Daily_Star/status/83938477...</td>\n",
       "      <td>http://bit.ly/2lWNDnt</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243887</th>\n",
       "      <td>2497991305</td>\n",
       "      <td>AUSTINLOVESBEER</td>\n",
       "      <td>Why men should support International Women’s D...</td>\n",
       "      <td>United States</td>\n",
       "      <td>English</td>\n",
       "      <td>3/8/2017 8:59</td>\n",
       "      <td>3/8/2017 9:00</td>\n",
       "      <td>41</td>\n",
       "      <td>34</td>\n",
       "      <td>175</td>\n",
       "      <td>...</td>\n",
       "      <td>Right</td>\n",
       "      <td>1</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>0</td>\n",
       "      <td>2497991305</td>\n",
       "      <td>839400290168135680</td>\n",
       "      <td>http://twitter.com/2497991305/statuses/8394002...</td>\n",
       "      <td>http://trib.al/xiMs3md</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243888</th>\n",
       "      <td>2497991305</td>\n",
       "      <td>AUSTINLOVESBEER</td>\n",
       "      <td>How we can rebuild trust in a UK divided by in...</td>\n",
       "      <td>United States</td>\n",
       "      <td>English</td>\n",
       "      <td>3/8/2017 8:59</td>\n",
       "      <td>3/8/2017 8:59</td>\n",
       "      <td>41</td>\n",
       "      <td>34</td>\n",
       "      <td>170</td>\n",
       "      <td>...</td>\n",
       "      <td>Right</td>\n",
       "      <td>1</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>0</td>\n",
       "      <td>2497991305</td>\n",
       "      <td>839400090582179840</td>\n",
       "      <td>http://twitter.com/2497991305/statuses/8394000...</td>\n",
       "      <td>http://trib.al/l3iyCVF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243889</th>\n",
       "      <td>2497991305</td>\n",
       "      <td>AUSTINLOVESBEER</td>\n",
       "      <td>John Humphrys accused of patronising Angela Ra...</td>\n",
       "      <td>United States</td>\n",
       "      <td>English</td>\n",
       "      <td>3/8/2017 8:59</td>\n",
       "      <td>3/8/2017 8:59</td>\n",
       "      <td>41</td>\n",
       "      <td>34</td>\n",
       "      <td>171</td>\n",
       "      <td>...</td>\n",
       "      <td>Right</td>\n",
       "      <td>1</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>0</td>\n",
       "      <td>2497991305</td>\n",
       "      <td>839400131325648896</td>\n",
       "      <td>http://twitter.com/2497991305/statuses/8394001...</td>\n",
       "      <td>http://bit.ly/2m0OQL7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243890</th>\n",
       "      <td>2497991305</td>\n",
       "      <td>AUSTINLOVESBEER</td>\n",
       "      <td>Fossilized poop found in 180-million-year-old ...</td>\n",
       "      <td>United States</td>\n",
       "      <td>English</td>\n",
       "      <td>3/8/2017 8:59</td>\n",
       "      <td>3/8/2017 8:59</td>\n",
       "      <td>41</td>\n",
       "      <td>34</td>\n",
       "      <td>174</td>\n",
       "      <td>...</td>\n",
       "      <td>Right</td>\n",
       "      <td>1</td>\n",
       "      <td>RightTroll</td>\n",
       "      <td>0</td>\n",
       "      <td>2497991305</td>\n",
       "      <td>839400253413437440</td>\n",
       "      <td>http://twitter.com/2497991305/statuses/8394002...</td>\n",
       "      <td>http://dailym.ai/2lV5BXf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190252 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        external_author_id           author  \\\n",
       "0       906000000000000000           10_GOP   \n",
       "1       906000000000000000           10_GOP   \n",
       "2       906000000000000000           10_GOP   \n",
       "3       906000000000000000           10_GOP   \n",
       "4       906000000000000000           10_GOP   \n",
       "...                    ...              ...   \n",
       "243886          2497991305  AUSTINLOVESBEER   \n",
       "243887          2497991305  AUSTINLOVESBEER   \n",
       "243888          2497991305  AUSTINLOVESBEER   \n",
       "243889          2497991305  AUSTINLOVESBEER   \n",
       "243890          2497991305  AUSTINLOVESBEER   \n",
       "\n",
       "                                                  content         region  \\\n",
       "0       \"We have a sitting Democrat US Senator on tria...        Unknown   \n",
       "1       Marshawn Lynch arrives to game in anti-Trump s...        Unknown   \n",
       "2       Daughter of fallen Navy Sailor delivers powerf...        Unknown   \n",
       "3       JUST IN: President Trump dedicates Presidents ...        Unknown   \n",
       "4       19,000 RESPECTING our National Anthem! #StandF...        Unknown   \n",
       "...                                                   ...            ...   \n",
       "243886  BREAKING: Killer avalanche sweeps three skiers...  United States   \n",
       "243887  Why men should support International Women’s D...  United States   \n",
       "243888  How we can rebuild trust in a UK divided by in...  United States   \n",
       "243889  John Humphrys accused of patronising Angela Ra...  United States   \n",
       "243890  Fossilized poop found in 180-million-year-old ...  United States   \n",
       "\n",
       "       language     publish_date   harvested_date  following  followers  \\\n",
       "0       English  10/1/2017 19:58  10/1/2017 19:59       1052       9636   \n",
       "1       English  10/1/2017 22:43  10/1/2017 22:43       1054       9637   \n",
       "2       English  10/1/2017 22:50  10/1/2017 22:51       1054       9637   \n",
       "3       English  10/1/2017 23:52  10/1/2017 23:52       1062       9642   \n",
       "4       English   10/1/2017 2:13   10/1/2017 2:13       1050       9645   \n",
       "...         ...              ...              ...        ...        ...   \n",
       "243886  English    3/8/2017 8:59    3/8/2017 8:59         41         34   \n",
       "243887  English    3/8/2017 8:59    3/8/2017 9:00         41         34   \n",
       "243888  English    3/8/2017 8:59    3/8/2017 8:59         41         34   \n",
       "243889  English    3/8/2017 8:59    3/8/2017 8:59         41         34   \n",
       "243890  English    3/8/2017 8:59    3/8/2017 8:59         41         34   \n",
       "\n",
       "        updates  ... account_type retweet  account_category new_june_2018  \\\n",
       "0           253  ...        Right       0        RightTroll             0   \n",
       "1           254  ...        Right       0        RightTroll             0   \n",
       "2           255  ...        Right       1        RightTroll             0   \n",
       "3           256  ...        Right       0        RightTroll             0   \n",
       "4           246  ...        Right       1        RightTroll             0   \n",
       "...         ...  ...          ...     ...               ...           ...   \n",
       "243886      173  ...        Right       1        RightTroll             0   \n",
       "243887      175  ...        Right       1        RightTroll             0   \n",
       "243888      170  ...        Right       1        RightTroll             0   \n",
       "243889      171  ...        Right       1        RightTroll             0   \n",
       "243890      174  ...        Right       1        RightTroll             0   \n",
       "\n",
       "           alt_external_id            tweet_id  \\\n",
       "0       905874659358453760  914580356430536707   \n",
       "1       905874659358453760  914621840496189440   \n",
       "2       905874659358453760  914623490375979008   \n",
       "3       905874659358453760  914639143690555392   \n",
       "4       905874659358453760  914312219952861184   \n",
       "...                    ...                 ...   \n",
       "243886          2497991305  839400198002503680   \n",
       "243887          2497991305  839400290168135680   \n",
       "243888          2497991305  839400090582179840   \n",
       "243889          2497991305  839400131325648896   \n",
       "243890          2497991305  839400253413437440   \n",
       "\n",
       "                                              article_url  \\\n",
       "0       http://twitter.com/905874659358453760/statuses...   \n",
       "1       http://twitter.com/905874659358453760/statuses...   \n",
       "2       http://twitter.com/905874659358453760/statuses...   \n",
       "3       http://twitter.com/905874659358453760/statuses...   \n",
       "4       http://twitter.com/905874659358453760/statuses...   \n",
       "...                                                   ...   \n",
       "243886  http://twitter.com/2497991305/statuses/8394001...   \n",
       "243887  http://twitter.com/2497991305/statuses/8394002...   \n",
       "243888  http://twitter.com/2497991305/statuses/8394000...   \n",
       "243889  http://twitter.com/2497991305/statuses/8394001...   \n",
       "243890  http://twitter.com/2497991305/statuses/8394002...   \n",
       "\n",
       "                                               tco1_step1  \\\n",
       "0       https://twitter.com/10_gop/status/914580356430...   \n",
       "1       https://twitter.com/damienwoody/status/9145685...   \n",
       "2       https://twitter.com/10_gop/status/913231923715...   \n",
       "3       https://twitter.com/10_gop/status/914639143690...   \n",
       "4       https://twitter.com/realDonaldTrump/status/914...   \n",
       "...                                                   ...   \n",
       "243886  https://twitter.com/Daily_Star/status/83938477...   \n",
       "243887                             http://trib.al/xiMs3md   \n",
       "243888                             http://trib.al/l3iyCVF   \n",
       "243889                              http://bit.ly/2m0OQL7   \n",
       "243890                           http://dailym.ai/2lV5BXf   \n",
       "\n",
       "                   tco2_step1 tco3_step1  \n",
       "0                         NaN        NaN  \n",
       "1                         NaN        NaN  \n",
       "2                         NaN        NaN  \n",
       "3                         NaN        NaN  \n",
       "4                         NaN        NaN  \n",
       "...                       ...        ...  \n",
       "243886  http://bit.ly/2lWNDnt        NaN  \n",
       "243887                    NaN        NaN  \n",
       "243888                    NaN        NaN  \n",
       "243889                    NaN        NaN  \n",
       "243890                    NaN        NaN  \n",
       "\n",
       "[190252 rows x 21 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cb8eb77-c785-44cb-aad0-74aefd0e147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "writePath = 'input.txt'\n",
    "with open(writePath, 'w') as f:\n",
    "    f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "efdb7e82-c1cc-42d0-aed2-e9b62134bcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  15829063\n",
      "\"We have a sitting Democrat US Senator on trial for corruption and you've barely heard a peep from the mainstream media.\" ~ @nedryun /nMarshawn Lynch arrives to game in anti-Trump shirt. Judging by his sagging pants the shirt should say Lynch vs. belt /nDaughter of fallen Navy Sailor delivers powerful monologue on anthem protests, burns her NFL packers gear.  #BoycottNFL /nJUST IN: President Trump dedicates Presidents Cup golf tournament trophy to the people of Florida, Texas and Puerto Rico. /n19,000 RESPECTING our National Anthem! #StandForOurAnthem🇺🇸 /nDan Bongino: \"Nobody trolls liberals better than Donald Trump.\" Exactly!  /n🐝🐝🐝 /n'@SenatorMenendez @CarmenYulinCruz Doesn't matter that CNN doesn't report on your crimes. This won't change the fact that you're going down.'/nAs much as I hate promoting CNN article, here they are admitting EVERYTHING Trump said about PR relief two days ago. /nAfter the 'genocide' remark from San Juan Mayor the narrative has changed though. @CNN fixes i\n",
      " !\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz{|}~",
      " ¡¢£¥§©«­®¯°±²´·¸º»¿ÀÂÃÄÇÉÐÑÖ×ØÜßàáâãäçèéëìíïðñòóöøùúûüýĆĉČčğİıŁłŋōšŦŸŽžƃƒƷƸǝȟɐɑɔɯɹɾʍʞʟʰʷʸ˚˜̵̶̷̡̢̘̙̜̟̠̣̤̥̦̩̪̫̬̮̯̰̱̲̳̹̻̼͇͈͉͍͔͕͖͚̀́̃̄̅̇̈̊̋̌̍̏̑̒̓̽̾͂͆͊͋͌͑͒͗ͣͤͥͧͨͩͬͭͮͯ̚͞͡ͅΩαλσωЂАБВГМНОПРСТУФЧЭЮЯабвгдежзийклмнопрстухцчшщыьэюяєѕіњѵ҉ҤүҽӜԵՏժհմվ׃اعغفَِಠღᎠᎥᎦᎪᎬᎮᎯᎶᎻᎽᎾᏁᏆᏊᏋᏘᏞᏫᴉᴹᵀḟ  ​‍‑–—―‘’‚“”„‡•․…′″‹‼‽‿⁉⁰₡₣₦₩€⃣℅ℓ№℠™ℱℴ→↓↔↘↙↩↪↷⇒⇩∂∆−√∞≠⊕⋆⌐⌚⌛⏩⏪⏰⏱⏳⒊⒍⒎⒏⒐⒑Ⓜ─╭╯╰╱█■▪▲▶▸►◀●◕◻◼☀☁☃☄★☆☇☎☑☔☕☘☛☜☝☞☟☠☡☣☥☦☪☭☮☯☹☺♀♂♈♕♛♠♡♥♦♨♪♫♬♻♿⚊⚌⚒⚓⚔⚖⚛⚜⚠⚡⚪⚫⚰⚽⚾⛄⛅⛈⛑⛓⛔⛪⛱⛳⛵⛽✂✅✈✊✋✌✍✎✏✒✓✔✖✝✞✡✦✨✪✰✵✿❄❌❎❑❒❓❔❕❗❣❤➊➋➌➍➎➏➕➖➙➜➝➠➡➤➥➪➰➵➿⠀⤴⤵⬅⬆⬇⬛⭐⭕　《》「」【】〰〽かツ・ㅤ卐秋ﷻ︎️－ＡＢＣＥＦＩＫＬＮＯＲＳＴＵＷＹ￼�𗂚🅰🅱🅴🅷🅼🅿🆃🆈🆒🆓🆕🆗🆘🆙🇦🇧🇨🇩🇪🇫🇬🇮🇯🇰🇱🇲🇳🇵🇷🇸🇹🇺🇻🇽🇾🇿🌄🌅🌈🌉🌊🌍🌎🌏🌐🌙🌚🌛🌞🌟🌩🌪🌬🌮🌱🌲🌴🌵🌷🌸🌹🌺🌻🌼🌿🍂🍃🍄🍅🍇🍉🍋🍌🍎🍏🍑🍓🍕🍖🍣🍦🍧🍨🍩🍫🍳🍵🍷🍸🍹🍺🍻🍾🍿🎂🎃🎆🎇🎈🎉🎊🎓🎖🎙🎠🎣🎤🎥🎧🎨🎩🎫🎬🎯🎵🎶🎸🎹🎺🎼🏀🏁🏅🏆🏈🏊🏎🏙🏛🏠🏡🏥🏨🏻🏼🏽🏾🏿🐀🐂🐃🐇🐈🐊🐍🐎🐏🐐🐑🐒🐔🐕🐘🐜🐝🐟🐥🐩🐬🐮🐯🐰🐱🐳🐵🐶🐷🐸🐻🐼🐾🐿👀👁👂👅👆👇👈👉👊👋👌👍👎👏👐👑👓👜👠👣👤👥👨👩👮👯👰👱👶👷👹👺👻👽👾👿💀💁💃💄💅💉💊💋💌💍💎💐💒💓💔💕💖💗💙💚💛💜💝💞💟💡💢💣💤💥💦💧💨💩💪💫💬💭💯💰💲💵💶💷💸💺💻💼💾💿📁📅📈📉📊📌📍📓📖📚📜📝📞📢📦📧📰📱📲📷📸📹📺📻📼📽🔁🔆🔈🔊🔌🔍🔎🔐🔑🔓🔔🔗🔘🔜🔝🔞🔟🔥🔨🔪🔫🔭🔱🔴🔵🔶🔷🔹🔻🕊🕯🕵🕶🕺🖍🖒🖕🖤🖥🗑🗞🗣🗳🗽🗿😀😁😂😃😄😅😆😇😈😉😊😋😌😍😎😏😐😑😒😓😔😕😖😘😙😚😛😜😝😞😟😠😡😢😣😤😥😨😩😫😬😭😮😯😱😲😳😴😵😷😸😹😻😾🙀🙁🙂🙃🙄🙇🙈🙉🙊🙋🙌🙍🙏🚀🚂🚃🚄🚅🚆🚇🚉🚊🚋🚑🚒🚓🚔🚗🚞🚣🚧🚨🚫🚮🚶🚿🛌🛍🛑🛒🛡🛣🛤🛩🛫🤐🤑🤓🤔🤕🤗🤘🤙🤛🤜🤞🤠🤡🤢🤣🤥🤦🤧🤨🤪🤮🤵🤷🤹🥂🥃🥇🥊🥜🥝🦁🦄🦅🦇🦈🦊🦋🦌🦍🦏\n",
      "1092\n"
     ]
    }
   ],
   "source": [
    "with open('input.txt', 'r', encoding='utf=8') as f:\n",
    "    text = f.read()\n",
    "print('length of dataset in characters: ', len(text))\n",
    "print(text[:1000])\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(\"\".join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9430c0e4-e6c6-481a-931d-ec6a90e238f4",
   "metadata": {},
   "source": [
    "# Tokenize Input Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15d0d65-c6f8-4f03-88fa-9e54a2fc4446",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi = {ch:i for i,ch in enumerate(chars)}\n",
    "itos = {i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos][i] for i in l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e708707-49b7-4d3f-8609-956be9726770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "data = torch.tesnor(encode(text), dtype = torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb78f3-74ed-4e46-b84a-c44ada3d8996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set train and validation split\n",
    "n = int(.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b9aee-eb63-4671-b218-e0ae65b655ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee909b3-87b3-455f-b619-dd6bc411e986",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fc9b8b-7ff1-43fc-8435-f3e85a761c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efade938-9e6b-4e4b-ae86-cc9a1904e6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a6f83c-93b1-4593-bd6d-92b051daddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554a4f36-2ae6-40e1-a651-8b801689b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel()\n",
    "model.load_state_dict(torch.load('gpt_model.pth'))\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay = .0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9270c28f-514c-480a-bda6-f81988312b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "min_valid_loss = 0.0\n",
    "losses = estimate_loss()\n",
    "min_valid_loss = losses['val']\n",
    "for iter in range(max_iters):\n",
    "    model.train()\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    losses = estimate_loss\n",
    "    if losses['val']<min_valid_loss:\n",
    "        print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{losses['val']:.6f}) \\t Saving The Model\n",
    "        torch.save(model.state_dict(), 'gpt_model.pth')\n",
    "        min_valid_loss = losses['val']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cafa584-8776-47d0-9b37-3d8d3560b6bf",
   "metadata": {},
   "source": [
    "# Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a4e2d0-5ce5-4d8f-b1d9-7671ce690818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "model.load_state_dict(torch.load('gpt_model.pth'))\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5b8aa1-8e3f-4c4c-9055-6890a42010e9",
   "metadata": {},
   "source": [
    "# Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c150f170-d2e3-488c-bcc3-0bbebcfcb9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(decode(m.generate(context, max_new_tokens=500)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
